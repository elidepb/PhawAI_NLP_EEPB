{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "emotion_classification_analysis_(13) (1)"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de Clasificación de Emociones en Tweets en Español"
      ],
      "metadata": {
        "id": "47-vXAhhcvHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importación de Librerías y Configuración Inicial\n",
        "\n",
        "Importo todas las librerías necesarias para el análisis exploratorio, preprocesamiento de datos y construcción del modelo."
      ],
      "metadata": {
        "id": "xGcBTQYUcvHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Conv1D, MaxPooling1D, Dense, Dropout,\n",
        "    GlobalMaxPooling1D, LayerNormalization, Attention\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "2OdtbLY9cvHc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:52.988585Z",
          "iopub.execute_input": "2025-09-01T16:10:52.989123Z",
          "iopub.status.idle": "2025-09-01T16:10:52.997363Z",
          "shell.execute_reply.started": "2025-09-01T16:10:52.989098Z",
          "shell.execute_reply": "2025-09-01T16:10:52.996632Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate -q"
      ],
      "metadata": {
        "id": "install-transformers-lib",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:52.999082Z",
          "iopub.execute_input": "2025-09-01T16:10:52.999941Z",
          "iopub.status.idle": "2025-09-01T16:10:56.294814Z",
          "shell.execute_reply.started": "2025-09-01T16:10:52.999916Z",
          "shell.execute_reply": "2025-09-01T16:10:56.293785Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Descarga y Carga del Dataset EmoEvent\n",
        "\n",
        "Descargo el dataset EmoEvent desde su repositorio de GitHub y cargo la versión en español."
      ],
      "metadata": {
        "id": "Al6TEXHQcvHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/fmplaza/EmoEvent.git\"\n",
        "repo_dir = \"EmoEvent\"\n",
        "\n",
        "# Descargar y descomprimir los embeddings de FastText en español si no existen\n",
        "fasttext_file = 'cc.es.300.vec'\n",
        "if not os.path.exists(fasttext_file):\n",
        "    print(\"Descargando embeddings de FastText...\")\n",
        "    os.system('wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz')\n",
        "    print(\"Descomprimiendo embeddings...\")\n",
        "    os.system('gunzip cc.es.300.vec.gz')\n",
        "else:\n",
        "    print(f\"El archivo de embeddings '{fasttext_file}' ya existe.\")\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(f\"Clonando repositorio desde {repo_url}...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "else:\n",
        "    print(f\"El repositorio '{repo_dir}' ya existe.\")\n",
        "\n",
        "data_dir = os.path.join(repo_dir, 'splits', 'es')\n",
        "files = {\n",
        "    'train': os.path.join(data_dir, 'train.tsv'),\n",
        "    'dev': os.path.join(data_dir, 'dev.tsv'),\n",
        "    'test': os.path.join(data_dir, 'test.tsv')\n",
        "}\n",
        "\n",
        "all_files_exist = True\n",
        "for split, filepath in files.items():\n",
        "    if not os.path.exists(filepath):\n",
        "        all_files_exist = False\n",
        "\n",
        "if all_files_exist:\n",
        "    print(\"Archivos del dataset listos para cargar.\")"
      ],
      "metadata": {
        "id": "zjrhQWWWcvHf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:56.295899Z",
          "iopub.execute_input": "2025-09-01T16:10:56.296178Z",
          "iopub.status.idle": "2025-09-01T16:10:56.304637Z",
          "shell.execute_reply.started": "2025-09-01T16:10:56.296143Z",
          "shell.execute_reply": "2025-09-01T16:10:56.303798Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filepath):\n",
        "    df = pd.read_csv(filepath, sep='\\t', header=0)\n",
        "    # Corregir los nombres de las columnas que están mal en el archivo original\n",
        "    df.rename(columns={'tweet': 'text', 'offensive': 'is_offensive', 'emotion': 'emotion_label'}, inplace=True)\n",
        "    # Reordenar y seleccionar las columnas deseadas\n",
        "    df = df[['event', 'text', 'emotion_label', 'is_offensive']]\n",
        "    df.columns = ['event', 'text', 'emotion', 'is_offensive']\n",
        "    return df\n",
        "\n",
        "train_df = load_dataset('EmoEvent/splits/es/train.tsv')\n",
        "dev_df = load_dataset('EmoEvent/splits/es/dev.tsv')\n",
        "test_df = load_dataset('EmoEvent/splits/es/test.tsv')\n",
        "\n",
        "full_df = pd.concat([train_df, dev_df, test_df], ignore_index=True)\n",
        "\n",
        "print(f\"Dataset completo cargado:\")\n",
        "print(f\"- Entrenamiento: {len(train_df)} tweets\")\n",
        "print(f\"- Desarrollo: {len(dev_df)} tweets\")\n",
        "print(f\"- Prueba: {len(test_df)} tweets\")\n",
        "print(f\"- Total: {len(full_df)} tweets\")\n",
        "\n",
        "display(full_df.head())"
      ],
      "metadata": {
        "id": "c1KoPxOncvHg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:56.305398Z",
          "iopub.execute_input": "2025-09-01T16:10:56.305633Z",
          "iopub.status.idle": "2025-09-01T16:10:56.383023Z",
          "shell.execute_reply.started": "2025-09-01T16:10:56.305607Z",
          "shell.execute_reply": "2025-09-01T16:10:56.382331Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Análisis Exploratorio de Datos (EDA)\n",
        "\n",
        "Realizo un análisis del dataset para comprender la distribución de emociones, eventos y características del texto."
      ],
      "metadata": {
        "id": "S3x_dBvPcvHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Información General y Limpieza del Dataset\n",
        "\n",
        "Examino la estructura del dataset, valores nulos y tipos de datos. También realizo una limpieza inicial."
      ],
      "metadata": {
        "id": "4GWxVjF8cvHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Forma del dataset original:\", full_df.shape)\n",
        "print(\"\\nValores nulos antes de la limpieza:\")\n",
        "print(full_df.isnull().sum())\n",
        "\n",
        "# Limpieza de valores nulos\n",
        "full_df.dropna(inplace=True)\n",
        "\n",
        "print(\"\\nForma del dataset después de la limpieza:\", full_df.shape)\n",
        "print(\"\\nValores nulos después de la limpieza:\")\n",
        "print(full_df.isnull().sum())\n",
        "\n",
        "# Estadísticas descriptivas del texto\n",
        "full_df['text_length'] = full_df['text'].str.len()\n",
        "full_df['word_count'] = full_df['text'].str.split().str.len()\n",
        "print(full_df[['text_length', 'word_count']].describe())\n",
        "\n",
        "# Calcular percentiles para la longitud de las secuencias\n",
        "percentile_95 = int(full_df['word_count'].quantile(0.95))\n",
        "percentile_99 = int(full_df['word_count'].quantile(0.99))\n",
        "\n",
        "print(f\"\\nAnálisis de Longitud de Secuencia:\")\n",
        "print(f\"El 95% de los tweets tienen {percentile_95} palabras o menos.\")\n",
        "print(f\"El 99% de los tweets tienen {percentile_99} palabras o menos.\")"
      ],
      "metadata": {
        "id": "Dpp5RDYXcvHh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:56.384962Z",
          "iopub.execute_input": "2025-09-01T16:10:56.385167Z",
          "iopub.status.idle": "2025-09-01T16:10:56.443019Z",
          "shell.execute_reply.started": "2025-09-01T16:10:56.385151Z",
          "shell.execute_reply": "2025-09-01T16:10:56.442277Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Distribución de Emociones\n",
        "\n",
        "Analizo la distribución de las categorías emocionales en el dataset."
      ],
      "metadata": {
        "id": "tQKTKuGdcvHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_counts = full_df['emotion'].value_counts()\n",
        "emotion_percentages = full_df['emotion'].value_counts(normalize=True) * 100\n",
        "\n",
        "emotion_stats = pd.DataFrame({\n",
        "    'Cantidad': emotion_counts,\n",
        "    'Porcentaje': emotion_percentages.round(2)\n",
        "})\n",
        "print(emotion_stats)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "emotion_counts.plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Distribución de Emociones')\n",
        "axes[0].set_xlabel('Emociones')\n",
        "axes[0].set_ylabel('Cantidad de Tweets')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1].pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%',\n",
        "           startangle=90, colors=sns.color_palette('husl', len(emotion_counts)))\n",
        "axes[1].set_title('Proporción de Emociones')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DuaNk46qcvHi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:56.443692Z",
          "iopub.execute_input": "2025-09-01T16:10:56.443906Z",
          "iopub.status.idle": "2025-09-01T16:10:56.760749Z",
          "shell.execute_reply.started": "2025-09-01T16:10:56.443881Z",
          "shell.execute_reply": "2025-09-01T16:10:56.760003Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Análisis por Eventos\n",
        "\n",
        "Exploro la distribución de emociones por evento."
      ],
      "metadata": {
        "id": "lnytYqlkcvHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_counts = full_df['event'].value_counts()\n",
        "print(f\"Número total de eventos únicos: {len(event_counts)}\")\n",
        "print(\"\\nTop 10 eventos más frecuentes:\")\n",
        "print(event_counts.head(10))\n",
        "\n",
        "top_events = event_counts.head(10).index\n",
        "emotion_event_matrix = pd.crosstab(full_df[full_df['event'].isin(top_events)]['event'],\n",
        "                                  full_df[full_df['event'].isin(top_events)]['emotion'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(emotion_event_matrix, annot=True, fmt='d', cmap='YlOrRd')\n",
        "plt.title('Distribución de Emociones por Evento (Top 10)')\n",
        "plt.xlabel('Emociones')\n",
        "plt.ylabel('Eventos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s0bSYYvacvHj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:56.761572Z",
          "iopub.execute_input": "2025-09-01T16:10:56.761856Z",
          "iopub.status.idle": "2025-09-01T16:10:57.121377Z",
          "shell.execute_reply.started": "2025-09-01T16:10:56.761835Z",
          "shell.execute_reply": "2025-09-01T16:10:57.120823Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Análisis de Características del Texto"
      ],
      "metadata": {
        "id": "ozcq7mMbcvHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(data=full_df, x='text_length', hue='emotion', multiple='stack', bins=30)\n",
        "plt.title('Distribución de Longitud por Emoción')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.boxplot(data=full_df, x='emotion', y='text_length')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Longitud de Texto por Emoción')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "text_stats = full_df.groupby('emotion')[['text_length', 'word_count']].agg(['mean', 'std', 'min', 'max'])\n",
        "print(text_stats.round(2))"
      ],
      "metadata": {
        "id": "obHSu-uzcvHj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:57.122331Z",
          "iopub.execute_input": "2025-09-01T16:10:57.122869Z",
          "iopub.status.idle": "2025-09-01T16:10:57.864229Z",
          "shell.execute_reply.started": "2025-09-01T16:10:57.122844Z",
          "shell.execute_reply": "2025-09-01T16:10:57.863258Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "def basic_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+|#', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "full_df['text_processed'] = full_df['text'].apply(basic_preprocess)\n",
        "full_df_cleaned = full_df.dropna(subset=['emotion']).copy()\n",
        "\n",
        "spanish_stopwords = set(stopwords.words('spanish'))\n",
        "additional_stopwords = {'rt', 'via', 'q', 'si', 'ya', 'ser', 'estar', 'tener', 'hacer'}\n",
        "spanish_stopwords.update(additional_stopwords)\n",
        "\n",
        "def get_top_words(texts, n=20):\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if word not in spanish_stopwords and len(word) > 2]\n",
        "        all_words.extend(words)\n",
        "    return Counter(all_words).most_common(n)\n",
        "\n",
        "for emotion in full_df_cleaned['emotion'].unique():\n",
        "    emotion_texts = full_df_cleaned[full_df_cleaned['emotion'] == emotion]['text_processed'].tolist()\n",
        "    top_words = get_top_words(emotion_texts, 10)\n",
        "    print(f\"\\n{emotion.upper()}:\")\n",
        "    for word, count in top_words:\n",
        "        print(f\"  {word}: {count}\")"
      ],
      "metadata": {
        "id": "M6c8XOk9cvHj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:57.865492Z",
          "iopub.execute_input": "2025-09-01T16:10:57.865876Z",
          "iopub.status.idle": "2025-09-01T16:10:59.432639Z",
          "shell.execute_reply.started": "2025-09-01T16:10:57.865846Z",
          "shell.execute_reply": "2025-09-01T16:10:59.431942Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Visualización con Nubes de Palabras"
      ],
      "metadata": {
        "id": "KHW91B75cvHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = full_df_cleaned['emotion'].unique()\n",
        "n_emotions = len(emotions)\n",
        "cols = 3\n",
        "rows = (n_emotions + cols - 1) // cols\n",
        "\n",
        "plt.figure(figsize=(18, 6 * rows))\n",
        "\n",
        "for i, emotion in enumerate(emotions):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "    emotion_texts = ' '.join(full_df_cleaned[full_df_cleaned['emotion'] == emotion]['text_processed'].dropna())\n",
        "\n",
        "    if emotion_texts.strip():\n",
        "        wordcloud = WordCloud(\n",
        "            width=400, height=300,\n",
        "            background_color='white',\n",
        "            stopwords=spanish_stopwords,\n",
        "            max_words=50,\n",
        "            colormap='viridis'\n",
        "        ).generate(emotion_texts)\n",
        "\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Nube de Palabras: {emotion.upper()}')\n",
        "        plt.axis('off')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f'No hay suficientes palabras para\\n \"{emotion.upper()}\"',\n",
        "                 horizontalalignment='center', verticalalignment='center',\n",
        "                 fontsize=12, color='gray')\n",
        "        plt.title(f'Nube de Palabras: {emotion.upper()}')\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kOsGvnV2cvHk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:10:59.433557Z",
          "iopub.execute_input": "2025-09-01T16:10:59.434227Z",
          "iopub.status.idle": "2025-09-01T16:11:02.004737Z",
          "shell.execute_reply.started": "2025-09-01T16:10:59.434206Z",
          "shell.execute_reply": "2025-09-01T16:11:02.003653Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocesamiento de Datos para Modelado\n",
        "\n",
        "Preparo los datos para el entrenamiento del modelo, incluyendo tokenización, codificación de etiquetas y división de conjuntos."
      ],
      "metadata": {
        "id": "cZwl-hjlcvHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Definir stopwords en español y añadir algunas personalizadas\n",
        "spanish_stopwords = set(stopwords.words('spanish'))\n",
        "# Eliminar palabras de negación de la lista de stopwords, ya que son importantes para el sentimiento\n",
        "negations = {'no', 'ni', 'ninguno', 'nada'}\n",
        "spanish_stopwords = spanish_stopwords - negations\n",
        "# Los tokens de marcador de posición para URL, MENTION, etc. todavía son útiles para eliminar\n",
        "additional_stopwords = {'rt', 'via', 'q', 'si', 'ya', 'ser', 'estar', 'tener', 'hacer', 'url', 'mention', 'hashtag', 'exclamation', 'question'}\n",
        "spanish_stopwords.update(additional_stopwords)\n",
        "\n",
        "def advanced_preprocess(text):\n",
        "    # Ensure text is a string before processing\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    return text\n",
        "\n",
        "# Combine train and dev dataframes before preprocessing\n",
        "train_combined = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "# Convert 'text' columns to string and fill potential NaN values before applying preprocess\n",
        "train_combined['text_clean'] = train_combined['text'].astype(str).fillna('').apply(advanced_preprocess)\n",
        "test_df['text_clean'] = test_df['text'].astype(str).fillna('').apply(advanced_preprocess)"
      ],
      "metadata": {
        "id": "B26aUZw2cvHk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:11:02.005612Z",
          "iopub.execute_input": "2025-09-01T16:11:02.005861Z",
          "iopub.status.idle": "2025-09-01T16:11:02.113383Z",
          "shell.execute_reply.started": "2025-09-01T16:11:02.005842Z",
          "shell.execute_reply": "2025-09-01T16:11:02.112808Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_cleaned = test_df.dropna(subset=['emotion']).copy()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_combined['emotion_encoded'] = label_encoder.fit_transform(train_combined['emotion'])\n",
        "test_df_cleaned['emotion_encoded'] = label_encoder.transform(test_df_cleaned['emotion'])\n",
        "\n",
        "emotion_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Feature Engineering\n",
        "event_one_hot = pd.get_dummies(train_combined['event'], prefix='event')\n",
        "is_offensive_feature = (train_combined['is_offensive'] == 'yes').astype(int)\n",
        "additional_features = pd.concat([event_one_hot, is_offensive_feature.rename('is_offensive')], axis=1)\n",
        "\n",
        "event_one_hot_test = pd.get_dummies(test_df_cleaned['event'], prefix='event')\n",
        "is_offensive_feature_test = (test_df_cleaned['is_offensive'] == 'yes').astype(int)\n",
        "additional_features_test = pd.concat([event_one_hot_test, is_offensive_feature_test.rename('is_offensive')], axis=1)\n",
        "\n",
        "# Align columns between train and test\n",
        "train_cols = additional_features.columns\n",
        "test_cols = additional_features_test.columns\n",
        "missing_in_test = set(train_cols) - set(test_cols)\n",
        "for c in missing_in_test:\n",
        "    additional_features_test[c] = 0\n",
        "missing_in_train = set(test_cols) - set(train_cols)\n",
        "for c in missing_in_train:\n",
        "    additional_features[c] = 0\n",
        "additional_features_test = additional_features_test[additional_features.columns]\n",
        "\n",
        "X_train_text = train_combined['text_clean'].values\n",
        "X_train_features = additional_features.values\n",
        "y_train = train_combined['emotion_encoded'].values\n",
        "X_test_text = test_df_cleaned['text_clean'].values\n",
        "X_test_features = additional_features_test.values\n",
        "y_test = test_df_cleaned['emotion_encoded'].values\n",
        "\n",
        "X_train_text_split, X_val_text_split, X_train_features_split, X_val_features_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_text, X_train_features, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")"
      ],
      "metadata": {
        "id": "t6w--ks_cvHk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:11:02.114108Z",
          "iopub.execute_input": "2025-09-01T16:11:02.114315Z",
          "iopub.status.idle": "2025-09-01T16:11:02.138251Z",
          "shell.execute_reply.started": "2025-09-01T16:11:02.114298Z",
          "shell.execute_reply": "2025-09-01T16:11:02.137731Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Tokenización y Preparación de Secuencias"
      ],
      "metadata": {
        "id": "NVbuveBTcvHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 15000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300  # Actualizado para coincidir con FastText\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    oov_token='<OOV>',\n",
        "    filters='',\n",
        "    lower=False\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(X_train_text_split)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text_split)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val_text_split)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train_split, num_classes)\n",
        "y_val_cat = tf.keras.utils.to_categorical(y_val_split, num_classes)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# --- Cargar Embeddings de FastText y Crear Matriz de Embeddings ---\n",
        "print('Cargando vectores de palabras de FastText...')\n",
        "embeddings_index = {}\n",
        "with open('cc.es.300.vec', 'r', encoding='utf-8') as f:\n",
        "    # La primera línea en el archivo .vec es el número de palabras y la dimensión\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print(f'Se encontraron {len(embeddings_index)} vectores de palabras.')\n",
        "\n",
        "# Crear una matriz de pesos para las palabras en los documentos de entrenamiento\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Las palabras no encontradas en el índice de embeddings serán ceros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "EQWlpbh7cvHl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:11:02.138915Z",
          "iopub.execute_input": "2025-09-01T16:11:02.139093Z",
          "iopub.status.idle": "2025-09-01T16:12:55.021091Z",
          "shell.execute_reply.started": "2025-09-01T16:11:02.139078Z",
          "shell.execute_reply": "2025-09-01T16:12:55.02021Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Construcción del Modelo Ensemble: BiLSTM + Attention"
      ],
      "metadata": {
        "id": "2pbqFQAscvHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Definición de la Arquitectura del Modelo"
      ],
      "metadata": {
        "id": "bR7bpY1jcvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "def create_multi_input_model(vocab_size, embedding_dim, max_length, num_classes, num_additional_features):\n",
        "    # Text input branch\n",
        "    input_text = Input(shape=(max_length,), name='input_text')\n",
        "    embedding = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=False,\n",
        "        name='embedding'\n",
        "    )(input_text)\n",
        "    embedding_dropout = Dropout(0.2, name='embedding_dropout')(embedding)\n",
        "\n",
        "    # BiLSTM layer\n",
        "    bilstm = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)\n",
        "    )(embedding_dropout)\n",
        "\n",
        "    # Attention layer\n",
        "    attention = Attention(name='attention')([bilstm, bilstm])\n",
        "\n",
        "    # Pooling\n",
        "    pool = GlobalMaxPooling1D(name='maxpool')(attention)\n",
        "\n",
        "    # Additional features input branch\n",
        "    input_features = Input(shape=(num_additional_features,), name='input_features')\n",
        "\n",
        "    # Concatenate text features with additional features\n",
        "    concatenated = concatenate([pool, input_features])\n",
        "\n",
        "    # Dense layers for classification\n",
        "    dense1 = Dense(128, activation='relu', name='dense_1')(concatenated)\n",
        "    dense1_dropout = Dropout(0.5, name='dense_dropout_1')(dense1)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax', name='output')(dense1_dropout)\n",
        "\n",
        "    model = Model(inputs=[input_text, input_features], outputs=output, name='BiLSTM_Attention_Features_Model')\n",
        "    return model\n",
        "\n",
        "model = create_multi_input_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    max_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_classes=num_classes,\n",
        "    num_additional_features=X_train_features.shape[1]\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wCiCyfpycvHn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:12:55.023516Z",
          "iopub.execute_input": "2025-09-01T16:12:55.023765Z",
          "iopub.status.idle": "2025-09-01T16:12:56.941209Z",
          "shell.execute_reply.started": "2025-09-01T16:12:55.023736Z",
          "shell.execute_reply": "2025-09-01T16:12:56.940657Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Compilación del Modelo"
      ],
      "metadata": {
        "id": "xVLPHpgOcvHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def focal_loss(gamma=2., alpha=.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1+K.epsilon())) - K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
        "    return focal_loss_fixed\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "optimizer = Adam(learning_rate=initial_learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "# --- Manejo de Desbalance de Clases con Class Weight ---\n",
        "class_weights_dict = {0: 1.4, 1: 3.0, 2: 4.0, 3: 0.7, 4: 0.5, 5: 1.2, 6: 2.0}\n",
        "\n",
        "print(\"Pesos de clase definidos manualmente:\")\n",
        "print(class_weights_dict)"
      ],
      "metadata": {
        "id": "gEtLgkfAcvHo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:12:56.941909Z",
          "iopub.execute_input": "2025-09-01T16:12:56.942143Z",
          "iopub.status.idle": "2025-09-01T16:12:56.959082Z",
          "shell.execute_reply.started": "2025-09-01T16:12:56.942114Z",
          "shell.execute_reply": "2025-09-01T16:12:56.958494Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Entrenamiento del Modelo"
      ],
      "metadata": {
        "id": "bcNiA14tcvHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 30\n",
        "N_SPLITS = 5\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "histories = []\n",
        "scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_text, y_train)):\n",
        "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
        "\n",
        "    # Split data for this fold\n",
        "    X_train_text_fold = X_train_text[train_idx]\n",
        "    X_val_text_fold = X_train_text[val_idx]\n",
        "    X_train_features_fold = X_train_features[train_idx].astype(np.float32)\n",
        "    X_val_features_fold = X_train_features[val_idx].astype(np.float32)\n",
        "    y_train_fold = y_train[train_idx]\n",
        "    y_val_fold = y_train[val_idx]\n",
        "\n",
        "    # Tokenize and pad text data for this fold\n",
        "    tokenizer.fit_on_texts(X_train_text_fold)\n",
        "    X_train_seq_fold = tokenizer.texts_to_sequences(X_train_text_fold)\n",
        "    X_val_seq_fold = tokenizer.texts_to_sequences(X_val_text_fold)\n",
        "    X_train_pad_fold = pad_sequences(X_train_seq_fold, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "    X_val_pad_fold = pad_sequences(X_val_seq_fold, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    y_train_cat_fold = tf.keras.utils.to_categorical(y_train_fold, num_classes)\n",
        "    y_val_cat_fold = tf.keras.utils.to_categorical(y_val_fold, num_classes)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = create_multi_input_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        num_classes=num_classes,\n",
        "        num_additional_features=X_train_features.shape[1]\n",
        "    )\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train_pad_fold, X_train_features_fold],\n",
        "        y_train_cat_fold,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=([X_val_pad_fold, X_val_features_fold], y_val_cat_fold),\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weights_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "    histories.append(history)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "    y_pred_proba = model.predict([X_test_pad, X_test_features.astype(np.float32)], batch_size=BATCH_SIZE, verbose=0) # Convert to float32\n",
        "    y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
        "    report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_, output_dict=True)\n",
        "    scores.append(report)\n",
        "\n",
        "# --- Aggregate and Display Results ---\n",
        "avg_accuracy = np.mean([s['accuracy'] for s in scores])\n",
        "avg_precision = np.mean([s['weighted avg']['precision'] for s in scores])\n",
        "avg_recall = np.mean([s['weighted avg']['recall'] for s in scores])\n",
        "avg_f1 = np.mean([s['weighted avg']['f1-score'] for s in scores])\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Weighted Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Weighted Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average Weighted F1-Score: {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "SygRZHmOcvHr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:12:56.959907Z",
          "iopub.execute_input": "2025-09-01T16:12:56.96015Z",
          "iopub.status.idle": "2025-09-01T16:25:30.670591Z",
          "shell.execute_reply.started": "2025-09-01T16:12:56.960125Z",
          "shell.execute_reply": "2025-09-01T16:25:30.669936Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Modelo 2: DeBERTa-v3 con Fine-tuning Avanzado\n",
        "\n",
        "Ahora, implementaremos un modelo de Transformer de última generación, DeBERTa-v3, y lo afinaremos para nuestra tarea de clasificación de emociones."
      ],
      "metadata": {
        "id": "deberta-title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, ClassLabel\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "\n",
        "# Load tokenizer\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# We will use the previously cleaned text\n",
        "train_dataset_hf = Dataset.from_pandas(train_combined[['text_clean', 'emotion_encoded']])\n",
        "test_dataset_hf = Dataset.from_pandas(test_df_cleaned[['text_clean', 'emotion_encoded']])\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return deberta_tokenizer(examples['text_clean'], padding='max_length', truncation=True, max_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Apply tokenization\n",
        "train_tokenized_dataset = train_dataset_hf.map(tokenize_function, batched=True)\n",
        "test_tokenized_dataset = test_dataset_hf.map(tokenize_function, batched=True)\n",
        "\n",
        "# Rename column to match model's expected input\n",
        "train_tokenized_dataset = train_tokenized_dataset.rename_column(\"emotion_encoded\", \"labels\")\n",
        "test_tokenized_dataset = test_tokenized_dataset.rename_column(\"emotion_encoded\", \"labels\")\n",
        "\n",
        "# Convert labels column to ClassLabel for stratification\n",
        "class_names = label_encoder.classes_.tolist()\n",
        "num_classes_deberta = len(class_names)\n",
        "train_tokenized_dataset = train_tokenized_dataset.cast_column(\"labels\", ClassLabel(num_classes=num_classes_deberta, names=class_names))\n",
        "test_tokenized_dataset = test_tokenized_dataset.cast_column(\"labels\", ClassLabel(num_classes=num_classes_deberta, names=class_names))\n",
        "\n",
        "# Split the training data into train and validation sets\n",
        "train_val_split = train_tokenized_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
        "train_dataset_deberta = train_val_split['train']\n",
        "val_dataset_deberta = train_val_split['test']\n",
        "\n",
        "print(\"Data prepared for DeBERTa model.\")\n",
        "print(f\"Train dataset size: {len(train_dataset_deberta)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset_deberta)}\")\n",
        "print(f\"Test dataset size: {len(test_tokenized_dataset)}\")"
      ],
      "metadata": {
        "id": "deberta-data-prep",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:25:30.67138Z",
          "iopub.execute_input": "2025-09-01T16:25:30.671646Z",
          "iopub.status.idle": "2025-09-01T16:25:50.807463Z",
          "shell.execute_reply.started": "2025-09-01T16:25:30.671619Z",
          "shell.execute_reply": "2025-09-01T16:25:50.806802Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta esta celda primero\n",
        "import os\n",
        "from huggingface_hub import logout\n",
        "\n",
        "# Desconectarse completamente\n",
        "try:\n",
        "    logout()\n",
        "    print(\"Desconectado de Hugging Face Hub\")\n",
        "except:\n",
        "    print(\"No estaba conectado\")\n",
        "\n",
        "# Configurar variables de entorno para modo offline\n",
        "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
        "\n",
        "print(\"Modo offline configurado\")"
      ],
      "metadata": {
        "id": "bZTTJVU8yC_8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:25:50.808225Z",
          "iopub.execute_input": "2025-09-01T16:25:50.808481Z",
          "iopub.status.idle": "2025-09-01T16:25:50.818288Z",
          "shell.execute_reply.started": "2025-09-01T16:25:50.808464Z",
          "shell.execute_reply": "2025-09-01T16:25:50.817524Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar modo offline y desactivar todas las conexiones\n",
        "import os\n",
        "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Crear mappings con tipos nativos de Python usando label_encoder\n",
        "id2label = {int(i): str(label) for i, label in enumerate(label_encoder.classes_)}\n",
        "label2id = {str(label): int(i) for i, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "# Load model sin parámetros de configuración que puedan causar problemas\n",
        "try:\n",
        "    model_deberta = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=num_classes_deberta,\n",
        "        local_files_only=True,\n",
        "        use_auth_token=False\n",
        "    ).to(device)\n",
        "\n",
        "    # Configurar los labels después de cargar el modelo\n",
        "    model_deberta.config.id2label = id2label\n",
        "    model_deberta.config.label2id = label2id\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error cargando modelo: {e}\")\n",
        "    # Intentar sin local_files_only si falla\n",
        "    model_deberta = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=num_classes_deberta,\n",
        "        use_auth_token=False\n",
        "    ).to(device)\n",
        "\n",
        "    model_deberta.config.id2label = id2label\n",
        "    model_deberta.config.label2id = label2id\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_deberta',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_deberta',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",  # Cambiado de evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_deberta,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_deberta,\n",
        "    eval_dataset=val_dataset_deberta,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting DeBERTa model training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "deberta-training",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:25:50.819642Z",
          "iopub.execute_input": "2025-09-01T16:25:50.819956Z",
          "iopub.status.idle": "2025-09-01T16:28:16.667613Z",
          "shell.execute_reply.started": "2025-09-01T16:25:50.819939Z",
          "shell.execute_reply": "2025-09-01T16:28:16.666798Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "print(\"Evaluating DeBERTa model on the test set...\")\n",
        "test_results_deberta = trainer.predict(test_tokenized_dataset)\n",
        "\n",
        "# Print classification report\n",
        "y_true_deberta = test_tokenized_dataset['labels']\n",
        "y_pred_deberta = np.argmax(test_results_deberta.predictions, axis=-1)\n",
        "emotion_names = list(label_encoder.classes_)\n",
        "\n",
        "print(\"\\\\n--- Classification Report for DeBERTa ---\")\n",
        "print(classification_report(y_true_deberta, y_pred_deberta, target_names=emotion_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm_deberta = confusion_matrix(y_true_deberta, y_pred_deberta)\n",
        "cm_normalized_deberta = cm_deberta.astype('float') / cm_deberta.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_normalized_deberta, annot=True, fmt='.3f', cmap='Blues', xticklabels=emotion_names, yticklabels=emotion_names)\n",
        "plt.title('Matriz de Confusión Normalizada - DeBERTa')\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "deberta-evaluation",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:28:16.668485Z",
          "iopub.execute_input": "2025-09-01T16:28:16.668781Z",
          "iopub.status.idle": "2025-09-01T16:28:20.472993Z",
          "shell.execute_reply.started": "2025-09-01T16:28:16.668738Z",
          "shell.execute_reply": "2025-09-01T16:28:20.47228Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform per-event analysis for DeBERTa\n",
        "test_results_df_deberta = test_df_cleaned.copy()\n",
        "test_results_df_deberta['predicted_emotion'] = label_encoder.inverse_transform(y_pred_deberta)\n",
        "test_results_df_deberta['correct_prediction'] = (test_results_df_deberta['emotion'] == test_results_df_deberta['predicted_emotion'])\n",
        "\n",
        "event_performance_deberta = test_results_df_deberta.groupby('event')['correct_prediction'].agg(['count', 'sum', 'mean']).round(4)\n",
        "event_performance_deberta.columns = ['total_tweets', 'correct_predictions', 'accuracy']\n",
        "event_performance_deberta = event_performance_deberta.sort_values('accuracy', ascending=False)\n",
        "\n",
        "significant_events_deberta = event_performance_deberta[event_performance_deberta['total_tweets'] >= 2]\n",
        "\n",
        "if not significant_events_deberta.empty:\n",
        "    print(\"=== RENDIMIENTO POR EVENTO (≥2 tweets) - DeBERTa ===\")\n",
        "    print(f\"Eventos analizados: {len(significant_events_deberta)}\")\n",
        "    display(significant_events_deberta)\n",
        "else:\n",
        "    print(\"No hay eventos con 2 o más tweets en el conjunto de prueba para un análisis significativo.\")"
      ],
      "metadata": {
        "id": "deberta-event-analysis",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T16:28:20.473911Z",
          "iopub.execute_input": "2025-09-01T16:28:20.474315Z",
          "iopub.status.idle": "2025-09-01T16:28:20.493599Z",
          "shell.execute_reply.started": "2025-09-01T16:28:20.47429Z",
          "shell.execute_reply": "2025-09-01T16:28:20.493044Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Modelo 3: Instruction-Tuning de un LLM (Flan-T5)\n",
        "\n",
        "Para el tercer modelo, exploraremos el fine-tuning de un Large Language Model (LLM) que ha sido pre-entrenado con instrucciones. Usaremos `google/flan-t5-base`. La idea es formatear nuestras muestras de entrenamiento como instrucciones que el modelo debe seguir."
      ],
      "metadata": {
        "id": "flan-t5-title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "LLM_MODEL_NAME = 'google/flan-t5-base'\n",
        "\n",
        "# Load tokenizer and model\n",
        "llm_tokenizer = T5Tokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "llm_model = T5ForConditionalGeneration.from_pretrained(LLM_MODEL_NAME).to(device)\n",
        "\n",
        "# PROBLEMA CORREGIDO: Prompt más simple y directo para T5\n",
        "def create_instructional_dataset(df):\n",
        "    instructions = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['text_clean']\n",
        "        emotion = row['emotion']\n",
        "        # Prompt más simple y directo - T5 funciona mejor con prompts cortos\n",
        "        prompt = f\"Classify emotion: {text}\"\n",
        "        instructions.append({'text': prompt, 'label': emotion})\n",
        "    return Dataset.from_list(instructions)\n",
        "\n",
        "train_instruction_dataset = create_instructional_dataset(train_combined)\n",
        "test_instruction_dataset = create_instructional_dataset(test_df_cleaned)\n",
        "\n",
        "# PROBLEMA CORREGIDO: Tokenización mejorada y más robusta\n",
        "def tokenize_t5(examples):\n",
        "    # Tokenize input\n",
        "    model_inputs = llm_tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=64,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        return_tensors=None  # Importante: no devolver tensors aquí\n",
        "    )\n",
        "\n",
        "    # Tokenize labels\n",
        "    with llm_tokenizer.as_target_tokenizer():\n",
        "        labels = llm_tokenizer(\n",
        "            examples['label'],\n",
        "            max_length=8,\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            return_tensors=None  # Importante: no devolver tensors aquí\n",
        "        )\n",
        "\n",
        "    # Asegurarse de que labels tenga la estructura correcta\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "train_tokenized_t5 = train_instruction_dataset.map(\n",
        "    tokenize_t5,\n",
        "    batched=True,\n",
        "    remove_columns=train_instruction_dataset.column_names  # Remover columnas originales\n",
        ")\n",
        "\n",
        "test_tokenized_t5 = test_instruction_dataset.map(\n",
        "    tokenize_t5,\n",
        "    batched=True,\n",
        "    remove_columns=test_instruction_dataset.column_names  # Remover columnas originales\n",
        ")\n",
        "\n",
        "# Split train into train and validation\n",
        "train_val_split_t5 = train_tokenized_t5.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset_t5 = train_val_split_t5['train']\n",
        "val_dataset_t5 = train_val_split_t5['test']\n",
        "\n",
        "print(\"Data prepared for Flan-T5 model.\")\n",
        "print(f\"Train dataset size: {len(train_dataset_t5)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset_t5)}\")\n",
        "print(f\"Test dataset size: {len(test_tokenized_t5)}\")\n",
        "\n",
        "# Verificar algunas muestras tokenizadas\n",
        "print(\"\\n--- Verificación de muestras tokenizadas ---\")\n",
        "for i in range(2):\n",
        "    sample = train_dataset_t5[i]\n",
        "    print(f\"Ejemplo {i+1}:\")\n",
        "    print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
        "    print(f\"Labels shape: {len(sample['labels'])}\")\n",
        "    print(f\"Input text (decoded): {llm_tokenizer.decode(sample['input_ids'], skip_special_tokens=True)}\")\n",
        "    print(f\"Label text (decoded): {llm_tokenizer.decode(sample['labels'], skip_special_tokens=True)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "flan-t5-data-prep",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:15:59.453498Z",
          "iopub.execute_input": "2025-09-01T17:15:59.453816Z",
          "iopub.status.idle": "2025-09-01T17:16:04.155797Z",
          "shell.execute_reply.started": "2025-09-01T17:15:59.453786Z",
          "shell.execute_reply": "2025-09-01T17:16:04.155051Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args_t5 = TrainingArguments(\n",
        "    output_dir='./results_t5',\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        "    logging_steps=100,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=None,\n",
        "    seed=42,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:16:28.522998Z",
          "iopub.execute_input": "2025-09-01T17:16:28.523248Z",
          "iopub.status.idle": "2025-09-01T17:16:28.552457Z",
          "shell.execute_reply.started": "2025-09-01T17:16:28.523231Z",
          "shell.execute_reply": "2025-09-01T17:16:28.551945Z"
        },
        "id": "jlJazfnxV9i5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Trainer\n",
        "import torch\n",
        "\n",
        "# Data collator correcto con configuración mejorada\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=llm_tokenizer,\n",
        "    model=llm_model,\n",
        "    label_pad_token_id=-100,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"  # Asegurar que devuelve tensors de PyTorch\n",
        ")\n",
        "\n",
        "# Crear el trainer\n",
        "trainer_t5 = Trainer(\n",
        "    model=llm_model,\n",
        "    args=training_args_t5,\n",
        "    train_dataset=train_dataset_t5,\n",
        "    eval_dataset=val_dataset_t5,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=llm_tokenizer\n",
        ")\n",
        "\n",
        "# Verificar que los datos están correctamente formateados antes del entrenamiento\n",
        "print(\"Verificando formato de datos...\")\n",
        "sample_batch = [train_dataset_t5[i] for i in range(2)]\n",
        "try:\n",
        "    collated = data_collator(sample_batch)\n",
        "    print(\"✓ Data collator funciona correctamente\")\n",
        "    print(f\"Batch input_ids shape: {collated['input_ids'].shape}\")\n",
        "    print(f\"Batch labels shape: {collated['labels'].shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error en data collator: {e}\")\n",
        "    raise e\n",
        "\n",
        "# PROBLEMA PRINCIPAL: ¡FALTABA ENTRENAR EL MODELO!\n",
        "print(\"Iniciando entrenamiento del modelo Flan-T5...\")\n",
        "trainer_t5.train()\n",
        "\n",
        "print(\"Entrenamiento completado. Guardando modelo...\")\n",
        "trainer_t5.save_model('./flan_t5_emotion_classifier')\n",
        "print(\"Modelo guardado exitosamente.\")"
      ],
      "metadata": {
        "id": "flan-t5-training",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:16:30.483205Z",
          "iopub.execute_input": "2025-09-01T17:16:30.483482Z",
          "iopub.status.idle": "2025-09-01T17:22:22.28769Z",
          "shell.execute_reply.started": "2025-09-01T17:16:30.483461Z",
          "shell.execute_reply": "2025-09-01T17:22:22.286839Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "def evaluate_t5_model():\n",
        "    print(\"Evaluando modelo Flan-T5 en el conjunto de prueba...\")\n",
        "\n",
        "    # Asegurar que el modelo esté en modo evaluación\n",
        "    llm_model.eval()\n",
        "\n",
        "    # Listas para almacenar predicciones y etiquetas reales\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    # Definir emociones válidas\n",
        "    valid_emotions = ['anger', 'sadness', 'joy', 'disgust', 'fear', 'surprise', 'offensive', 'other']\n",
        "\n",
        "    # Función para limpiar y mapear predicciones\n",
        "    def clean_prediction(pred_text):\n",
        "        pred_clean = pred_text.strip().lower()\n",
        "        # Si la predicción exacta está en las emociones válidas, usarla\n",
        "        if pred_clean in valid_emotions:\n",
        "            return pred_clean\n",
        "        # Si no, buscar si alguna emoción válida está contenida en la predicción\n",
        "        for emotion in valid_emotions:\n",
        "            if emotion in pred_clean:\n",
        "                return emotion\n",
        "        # Si no se encuentra nada, clasificar como 'other'\n",
        "        return 'other'\n",
        "\n",
        "    # Procesar muestras en lotes pequeños\n",
        "    batch_size = 16\n",
        "    total_samples = len(test_instruction_dataset)\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "        end_idx = min(i + batch_size, total_samples)\n",
        "        batch_texts = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # Preparar el batch\n",
        "        for j in range(i, end_idx):\n",
        "            batch_texts.append(test_instruction_dataset[j]['text'])\n",
        "            batch_labels.append(test_instruction_dataset[j]['label'])\n",
        "\n",
        "        try:\n",
        "            # Tokenizar el batch\n",
        "            inputs = llm_tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=64\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Generar predicciones\n",
        "            with torch.no_grad():\n",
        "                outputs = llm_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=8,\n",
        "                    num_beams=2,\n",
        "                    do_sample=False,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=llm_tokenizer.pad_token_id,\n",
        "                    eos_token_id=llm_tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Decodificar predicciones\n",
        "            batch_predictions = llm_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "            # Procesar predicciones\n",
        "            for pred_raw in batch_predictions:\n",
        "                cleaned_pred = clean_prediction(pred_raw)\n",
        "                all_predictions.append(cleaned_pred)\n",
        "\n",
        "            # Agregar etiquetas verdaderas\n",
        "            all_true_labels.extend(batch_labels)\n",
        "\n",
        "            # Mostrar progreso\n",
        "            if (end_idx) % 100 == 0 or end_idx == total_samples:\n",
        "                print(f\"Procesado {end_idx}/{total_samples} muestras...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en batch {i//batch_size + 1}: {e}\")\n",
        "            # En caso de error, agregar predicciones por defecto\n",
        "            for _ in range(len(batch_texts)):\n",
        "                all_predictions.append('other')\n",
        "            all_true_labels.extend(batch_labels)\n",
        "            continue\n",
        "\n",
        "    # Verificar que las longitudes coincidan\n",
        "    if len(all_predictions) != len(all_true_labels):\n",
        "        min_len = min(len(all_predictions), len(all_true_labels))\n",
        "        all_predictions = all_predictions[:min_len]\n",
        "        all_true_labels = all_true_labels[:min_len]\n",
        "        print(f\"Ajustadas longitudes a {min_len} muestras\")\n",
        "\n",
        "    # Mostrar algunas predicciones de ejemplo\n",
        "    print(f\"\\n--- Ejemplos de predicciones ---\")\n",
        "    for i in range(min(10, len(all_predictions))):\n",
        "        print(f\"Texto: {test_instruction_dataset[i]['text'][:100]}...\")\n",
        "        print(f\"Real: {all_true_labels[i]} | Predicción: {all_predictions[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Calcular métricas\n",
        "    print(f\"\\n--- Reporte de Clasificación para Flan-T5 ---\")\n",
        "    print(f\"Total de muestras procesadas: {len(all_true_labels)}\")\n",
        "\n",
        "    # Verificar distribución de predicciones\n",
        "    from collections import Counter\n",
        "    pred_dist = Counter(all_predictions)\n",
        "    true_dist = Counter(all_true_labels)\n",
        "\n",
        "    print(f\"\\nDistribución de predicciones: {dict(pred_dist)}\")\n",
        "    print(f\"Distribución de etiquetas reales: {dict(true_dist)}\")\n",
        "\n",
        "    # Reporte de clasificación\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    report = classification_report(\n",
        "        all_true_labels,\n",
        "        all_predictions,\n",
        "        labels=valid_emotions,\n",
        "        target_names=valid_emotions,\n",
        "        zero_division=0\n",
        "    )\n",
        "    print(report)\n",
        "\n",
        "    # Matriz de confusión\n",
        "    try:\n",
        "        cm = confusion_matrix(all_true_labels, all_predictions, labels=valid_emotions)\n",
        "\n",
        "        # Normalizar matriz de confusión\n",
        "        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
        "\n",
        "        # Crear gráfico\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm_normalized,\n",
        "                   annot=True,\n",
        "                   fmt='.3f',\n",
        "                   cmap='Blues',\n",
        "                   xticklabels=valid_emotions,\n",
        "                   yticklabels=valid_emotions)\n",
        "        plt.title('Matriz de Confusión Normalizada - Flan-T5')\n",
        "        plt.xlabel('Predicción')\n",
        "        plt.ylabel('Etiqueta Real')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al crear matriz de confusión: {e}\")\n",
        "\n",
        "    return all_predictions, all_true_labels\n",
        "\n",
        "# Ejecutar evaluación\n",
        "predictions, true_labels = evaluate_t5_model()"
      ],
      "metadata": {
        "id": "flan-t5-evaluation",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:24:28.817031Z",
          "iopub.execute_input": "2025-09-01T17:24:28.817584Z",
          "iopub.status.idle": "2025-09-01T17:24:38.71399Z",
          "shell.execute_reply.started": "2025-09-01T17:24:28.81756Z",
          "shell.execute_reply": "2025-09-01T17:24:38.713241Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Generando predicciones con el modelo Flan-T5 entrenado...\")\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "trained_model_path = './flan_t5_emotion_classifier'\n",
        "\n",
        "try:\n",
        "    # Intentar cargar el modelo entrenado\n",
        "    trained_model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "    trained_tokenizer = T5Tokenizer.from_pretrained(trained_model_path)\n",
        "    print(\"✓ Modelo entrenado cargado exitosamente\")\n",
        "except:\n",
        "    # Si no se puede cargar, usar el modelo del trainer\n",
        "    trained_model = trainer_t5.model\n",
        "    trained_tokenizer = llm_tokenizer\n",
        "    print(\"✓ Usando modelo del trainer actual\")\n",
        "\n",
        "# Mover modelo a dispositivo\n",
        "trained_model = trained_model.to(device)\n",
        "trained_model.eval()\n",
        "\n",
        "def predict_emotion_t5(text, model, tokenizer, max_length=64):\n",
        "    \"\"\"Predice la emoción de un texto usando el modelo T5 entrenado\"\"\"\n",
        "    # Crear el prompt igual que en entrenamiento\n",
        "    prompt = f\"Classify emotion: {text}\"\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Generar predicción\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=8,  # Longitud máxima para las emociones\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decodificar la predicción\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return prediction.strip()\n",
        "\n",
        "# Generar predicciones para el conjunto de prueba\n",
        "print(\"Generando predicciones para el conjunto de prueba...\")\n",
        "final_predictions_t5 = []\n",
        "\n",
        "# Usar tqdm para mostrar progreso\n",
        "for i, row in tqdm(test_df_cleaned.iterrows(), total=len(test_df_cleaned), desc=\"Predicciones\"):\n",
        "    try:\n",
        "        prediction = predict_emotion_t5(row['text_clean'], trained_model, trained_tokenizer)\n",
        "        final_predictions_t5.append(prediction)\n",
        "    except Exception as e:\n",
        "        print(f\"Error en predicción {i}: {e}\")\n",
        "        # En caso de error, usar una predicción por defecto\n",
        "        final_predictions_t5.append(\"neutral\")\n",
        "\n",
        "print(f\"✓ Predicciones completadas: {len(final_predictions_t5)}\")\n",
        "\n",
        "# Verificar algunas predicciones\n",
        "print(\"\\n--- Verificación de predicciones ---\")\n",
        "for i in range(min(5, len(test_df_cleaned))):\n",
        "    print(f\"Texto: {test_df_cleaned.iloc[i]['text_clean'][:100]}...\")\n",
        "    print(f\"Emoción real: {test_df_cleaned.iloc[i]['emotion']}\")\n",
        "    print(f\"Predicción: {final_predictions_t5[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Estadísticas básicas de las predicciones\n",
        "from collections import Counter\n",
        "prediction_counts = Counter(final_predictions_t5)\n",
        "print(\"\\n--- Distribución de predicciones ---\")\n",
        "for emotion, count in prediction_counts.most_common():\n",
        "    print(f\"{emotion}: {count} ({count/len(final_predictions_t5)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nPredicciones únicas encontradas: {len(prediction_counts)}\")\n",
        "print(\"¡Predicciones generadas exitosamente! Ahora puedes ejecutar el análisis por eventos.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:29:23.877235Z",
          "iopub.execute_input": "2025-09-01T17:29:23.877531Z",
          "iopub.status.idle": "2025-09-01T17:30:53.960287Z",
          "shell.execute_reply.started": "2025-09-01T17:29:23.877506Z",
          "shell.execute_reply": "2025-09-01T17:30:53.959465Z"
        },
        "id": "2fh1skkjV9i6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform per-event analysis for Flan-T5\n",
        "test_results_df_t5 = test_df_cleaned.copy()\n",
        "test_results_df_t5['predicted_emotion'] = final_predictions_t5\n",
        "test_results_df_t5['correct_prediction'] = (test_results_df_t5['emotion'] == test_results_df_t5['predicted_emotion'])\n",
        "\n",
        "event_performance_t5 = test_results_df_t5.groupby('event')['correct_prediction'].agg(['count', 'sum', 'mean']).round(4)\n",
        "event_performance_t5.columns = ['total_tweets', 'correct_predictions', 'accuracy']\n",
        "event_performance_t5 = event_performance_t5.sort_values('accuracy', ascending=False)\n",
        "\n",
        "significant_events_t5 = event_performance_t5[event_performance_t5['total_tweets'] >= 2]\n",
        "\n",
        "if not significant_events_t5.empty:\n",
        "    print(\"=== RENDIMIENTO POR EVENTO (≥2 tweets) - Flan-T5 ===\")\n",
        "    print(f\"Eventos analizados: {len(significant_events_t5)}\")\n",
        "    display(significant_events_t5)\n",
        "else:\n",
        "    print(\"No hay eventos con 2 o más tweets en el conjunto de prueba para un análisis significativo.\")"
      ],
      "metadata": {
        "id": "flan-t5-event-analysis",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-01T17:31:32.683359Z",
          "iopub.execute_input": "2025-09-01T17:31:32.68364Z",
          "iopub.status.idle": "2025-09-01T17:31:32.698318Z",
          "shell.execute_reply.started": "2025-09-01T17:31:32.683619Z",
          "shell.execute_reply": "2025-09-01T17:31:32.697276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluación de los modelos"
      ],
      "metadata": {
        "id": "HbtZ7hmIcvHt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jD4uSUqLV9i6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
